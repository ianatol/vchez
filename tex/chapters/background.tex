\chapter{Background}
\section{Compiler Correctness}
"Can you trust your compiler?"

This quote begins the paper on the CompCert \cite{leroy_formal_2009} project, which provides a verified compiler for a large subset of C. As previously mentioned, an answer of "yes" to this question is assumed by many proofs about the behavior of programs. Proofs of compiler correctness aim to formally verify the validity of this assumption. However, for large languages, which have correspondingly large compilers, these proofs are quite complex --- the CompCert project totals around 100,000 lines of code in the Coq proof assistant.

We will begin with some history on compiler correctness proofs. Then, we will show some recent examples of these sorts of proofs and how they are accomplished. At the end, we will come back to CompCert, as its influence and impact on the area cannot be understated.

% Add
%   citations
%   history of proof assistants
%   Modern examples of correctness proofs
\subsection{History}
The first known formal proof of compiler correctness comes from John McCarthy and James Painter \cite{mccarthy_correctness_1967}. In their 1967 paper, they prove that a compiler that translates simple arithmetic expressions to machine code is correct. Despite the simplicity of the example source language, the paper is very important in that it sets up a methodology for computational proofs of compiler correctness. Indeed, their notion of equivalence of expressions before and after compilation is quite similar to our definition of equivalence (see section \ref{equiv}). Additionally, their method of proof by structural induction of expressions is the predominant proof strategy for reasoning about all programs in a language.

McCarthy and Painter's proof was intentionally simple, so much so that it was able to be manually formulated and checked. However, when dealing with larger languages, case analysis of language expressions quickly generates too large of a proof to keep track of by hand. Because of this complexity, modern proofs of compiler correctness universally utilize programs called proof checkers that embed mathematical systems and can computationally verify logical consistency of theorems defined within them. These checkers are necessary to aid with management of proofs at such a large scale. As such, their creation and development has been strongly connected to the progress of compiler correctness proofs.

\subsubsection{Proof Checkers, Intuitionistic Logic, and Mechanized Proofs}
The first large-scale attempt to \textit{mechanize} mathematics, or formally define mathematics in a way tractable by a computer, was Nicolaas Govert de Bruijn's Automath language \cite{x}. In the Automath language, theorems are defined as types, and proofs consist of showing that these types are inhabited by some value. This means that the definitions and proofs of theorems within Automath's logic can be encoded as a computer program. This relationship between programs and logic is also at the core of the Curry-Howard correspondence between deductive logic and the simply-typed Lambda Calculus.

Research into these sorts of program-logic relations continued into the 70s and 80s, with the development of the \textit{Intuitionistic Theory of Types} \cite{x} by Per Martin-LÃ¶f and the polymorphic Lambda calculus $\lambda_\omega$ \cite{x}. These theories also rely on the Curry-Howard correspondence to tie their computational systems to \textit{intuitionistic logic}. 

Intuitionistic logic is a kind of logical system that supports proof by construction. That is, to prove that a statement $A \xrightarrow{} B$ is true in an intuitionistic logic, one must take evidence of the truth of A and transform it using the axioms of the logic into evidence that B is true. One important property of these intuitionistic logics that follows their constructive foundation is that the law of the excluded middle is not true in these systems. 

As a reminder, the law of the excluded middle says that for all statements, that statement is either true or false (i.e. $A \lor \neg A$). This means that in logic systems that include the law of excluded middle, proof of a statement not being true is the same as a proof of that statement being false. While this seems obvious or tautological, it is not the case in intuitionistic logic. Instead, in intuitionistic logics, proof of a statement being true requires directly showing that that statement is true. Allowing the law of the excluded middle, as in classical logic, means that proofs of statements can sometimes be indirect. For example, proving a statement by contradiction is an indirect proof --- we know that the statement is true only because it can't be false. 

By not including the law of the excluded middle, intuitionistic logic is able to have fewer axioms and require more rigorous and direct proof of statements within it. It is for these reasons, as well as the natural connection between intuitionistic logic and dependent type theory, that intuitionistic logic was chosen by Thierry Coquand as the foundation for the Calculus of Inductive Constructions. This system combines Intuitionistic Type Theory and the polymorphic Lambda Calculus into a single calculus that also provides support for writing specifications that automatically come equipped with powerful induction principles.. 

While the Calculus of Inductive Constructions is powerful, it is unwieldy and hard to manually construct large programs with. For this reason, the Coq proof assistant \cite{x} was devised. An extension of the original Automath language, Coq embeds the Calculus of Inductive Constructions, and also provides a high level tactics language on top of the core calculus. This tactics language provides automation in the form of syntactic sugar and algorithmic search of core calculus expressions to assist in the construction of large proof terms. This tactics language greatly increase the size of proofs that Coq can handle --- indeed, Coq programmers spend the majority of their time writing, configuring, and refactoring proofs using the tactics language, so at a much higher level than using the calculus itself. 

Because of its support for automation, its history, and a large amount of libraries and community support, Coq is a natural choice for large-scale mechanized proofs. One example of this is the proof of the four color theorem. This theorem was famously unsolved until Coq's automation features made its extensive case analysis proof feasible \cite{x}. Other projects realized in Coq include the Univalent Foundations project, which attempts to build a foundation for mathematics based on a type theory, and the CompCert project.
\subsection{CompCert}
CompCert is a verified C compiler written in Coq. It is one of the biggest verified compilers, and its proof is therefore very important to the field. Led by Xavier Leroy, the project took a long time and involved many people. However, the importance of a verified C compiler cannot be understated. The vast majority of embedded systems utilize C, and this is an area where formal verification is both needed and often used in industrial settings \cite{x}.
\section{Scheme}
Scheme is a functional language based on LISP, with a functional core very similar to the call-by-value Lambda Calculus.
\subsection{The Lambda Calculus, LISP, and ISWIM}
The Lambda Calculus was initially created by Alonzo Church and his students as a way to model mathematical notions, but it was eventually understood as a powerful abstraction that could serve as a basis for practical programming languages. Indeed, most of our modern programming languages have a basis in the lambda calculus. As such, it is very important and has been well-studied. The Lambda Calculus at its core is incredibly simple, with just 3 expressions and 2 rules for rewriting. This means that the Lambda Calculus is easy to extend to include additional features.

LISP was originally created to understand AI. The language is based primarily on linked-lists and includes some basic primitives to work with lists. However, one important feature of LISP is its homoiconity. This means that programs and their representation by the compiler (the abstract syntax tree) are the same, which means that programs can be handled just as normal data would be in the language.

By combining these two languages together, Scheme forms a list-based functional language that is still quite simple and extensible. Scheme combines the homoiconity of LISP with the simple but powerful form of functional abstraction from the Lambda Calculus. Additionally, it allows for developers to write macros, which extend the syntax of the language. Therefore, is easy to write domain specific languages using Scheme for various projects. 

Bridging the gap between Scheme and the Lambda Calculus is a language known as ISWIM. Extending the Lambda Calculus to include mutable variables, ISWIM is an abstract language better suited to implementation and extension into an actual usable progrmaming language than the Lambda Calculus. Scheme is largely based on ISWIM, which explains its relation to the Lambda Calculus, but is also the source of some of its differences from the very abstract mathematical model.

Finally, Scheme has a language standard in the form of the Revised$^n$ Report on the Algorithmic Language Scheme (R\textit{n}RS). R5RS was a longtime standard for the language, with some disagreement on the direction taken by R6RS and especially by R7RS. For this project, we decided to use R6RS because the Chez Scheme compiler is written in and compiles R6RS Scheme to machine code.
\subsection{Scheme Verification}
Because of its age and roots in mathematical systems, Scheme has been previously approached from the perspective of formal verification. Some of the relevant projects are listed below, with a description of their goals and results.
\subsection{The Chez Scheme compiler}
The Chez Scheme compiler is written in R6RS Scheme. However, it notably utilizes the Nanopass framework at its foundations. The Nanopass framework is a means of quickly defining many intermediate languages and small passes between them. Some examples of Nanopass language defintions and pass definitions are shown below.

We specifically chose to target the Chez Scheme compiler for verification because of the Nanopass framework. Since passes are distinctly seperated and small in size by design, correctness proofs of individual passes should be simpler, and also are able to be composed to make a proof of correctness of the entire compiler. For this project, we build a framework for reasoning about Scheme and use it to prove correctness of one pass of the Chez Scheme compiler.

